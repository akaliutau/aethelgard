{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "796GSj38srYU",
        "outputId": "90af8641-32e2-4a15-f7f8-9655875eca06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'chexpert' dataset.\n",
            "Path to dataset files: /kaggle/input/chexpert\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"ashery/chexpert\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "def create_dataset_subset(\n",
        "    csv_path='train.csv',\n",
        "    output_csv_path='train.csv',\n",
        "    source_dir='.',\n",
        "    dest_dir='/content/dataset_subset',\n",
        "    num_patients=100\n",
        "):\n",
        "    # 1. Load the original CSV dataset\n",
        "    print(f\"Loading {csv_path}...\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df.head()\n",
        "\n",
        "    # 2. Extract the patient ID from the 'Path' column\n",
        "    # Example Path: CheXpert-v1.0-small/train/patient00001/study1/view1_frontal.jpg\n",
        "    # Using split('/'), the patient ID is usually at index 2 (if the path starts with CheXpert...)\n",
        "    # We can use a regex to be more robust, extracting 'patient\\d+'\n",
        "    df['patient_id'] = df['Path'].str.extract(r'(patient\\d+)')\n",
        "\n",
        "    # 3. Get the first 'num_patients' unique patient IDs\n",
        "    unique_patients = df['patient_id'].dropna().unique()\n",
        "    first_n_patients = unique_patients[:num_patients]\n",
        "    print(f\"Filtering dataset for the first {len(first_n_patients)} unique patients...\")\n",
        "\n",
        "    # 4. Filter the dataframe\n",
        "    df_subset = df[df['patient_id'].isin(first_n_patients)].copy()\n",
        "\n",
        "    # Drop the temporary 'patient_id' column to keep the exact same schema\n",
        "    df_subset = df_subset.drop(columns=['patient_id'])\n",
        "\n",
        "    # 5. Copy the image files to the new location mirroring the file structure\n",
        "    print(f\"Copying images to {dest_dir}...\")\n",
        "    copied_count = 0\n",
        "    missing_files = []\n",
        "\n",
        "    for relative_path in df_subset['Path']:\n",
        "        src_path = os.path.join(source_dir, relative_path.replace('CheXpert-v1.0-small/', ''))\n",
        "        dst_path = os.path.join(dest_dir, relative_path)\n",
        "\n",
        "        # Create destination directories if they don't exist\n",
        "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
        "\n",
        "        # Copy file if it exists in the source\n",
        "        if os.path.exists(src_path):\n",
        "            if not os.path.exists(dst_path):\n",
        "                shutil.copy2(src_path, dst_path)\n",
        "            copied_count += 1\n",
        "        else:\n",
        "            missing_files.append(src_path)\n",
        "\n",
        "    print(f\"Copied {copied_count} files.\")\n",
        "    if missing_files:\n",
        "        print(f\"Warning: {len(missing_files)} files were listed in the CSV but not found in the source directory.\")\n",
        "\n",
        "    # 6. Save the new CSV file with exactly the same schema and relative paths\n",
        "    df_subset.to_csv(output_csv_path, index=False)\n",
        "    print(f\"Saved subset CSV to {output_csv_path} with {len(df_subset)} records.\")\n"
      ],
      "metadata": {
        "id": "8MwReVgZtmfw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zip_and_download(folder_path, zip_filename=\"dataset_subset\"):\n",
        "    \"\"\"Zips the specified folder and attempts to download it.\"\"\"\n",
        "    print(f\"Zipping '{folder_path}' into '{zip_filename}.zip'...\")\n",
        "\n",
        "    # Create the zip archive\n",
        "    shutil.make_archive(zip_filename, 'zip', folder_path)\n",
        "    print(\"Zip file created successfully.\")\n",
        "\n",
        "    # Attempt to download if running in Google Colab\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print(\"Google Colab environment detected. Initiating download...\")\n",
        "        files.download(f\"{zip_filename}.zip\")\n",
        "    except ImportError:\n",
        "        # Provide the absolute path for local environments\n",
        "        abs_path = os.path.abspath(f\"{zip_filename}.zip\")\n",
        "        print(f\"Local environment detected. Your dataset is ready at:\\n{abs_path}\")"
      ],
      "metadata": {
        "id": "QPP4GEsqxgJb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_csv = Path(path) / 'train.csv'\n",
        "create_dataset_subset(\n",
        "        csv_path=str(path_csv),\n",
        "        output_csv_path='/content/dataset_subset/train.csv',\n",
        "        source_dir=path,               # Set to your original dataset root folder\n",
        "        dest_dir='/content/dataset_subset',  # New folder to place the copied structure\n",
        "        num_patients=100\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj15JbiPvL6O",
        "outputId": "05169b85-b927-41c5-90a4-ded326856858"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading /kaggle/input/chexpert/train.csv...\n",
            "Filtering dataset for the first 100 unique patients...\n",
            "Copying images to /content/dataset_subset...\n",
            "Copied 362 files.\n",
            "Saved subset CSV to /content/dataset_subset/train.csv with 362 records.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_and_download(folder_path='/content/dataset_subset', zip_filename='chexpert')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "yTJvS5VbxikI",
        "outputId": "e50b00cb-f4c4-4163-9930-7093e6325e66"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zipping '/content/dataset_subset' into 'chexpert.zip.zip'...\n",
            "Zip file created successfully.\n",
            "Google Colab environment detected. Initiating download...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_817eae76-03f1-4369-9b5e-b25d9dc38360\", \"chexpert.zip.zip\", 18158341)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}